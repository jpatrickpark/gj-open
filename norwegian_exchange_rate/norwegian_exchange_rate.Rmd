---
title: "Norwegian Kroner to US Dollar"
author: "Joe"
date: "10/2/2017"
output: html_document
---

# Question: 
What will the Norwegian Kroner to one U.S. Dollar daily exchange rate be on 31 October 2017?

# Description: 
This question will be resolved using the daily rate reported by Federal Reserve Economic Data (FRED) for the date of interest. Question will be suspended the day before the date of interest and resolved when the data is released, usually on Monday of the following week.


```{r}
# Set the working directory to the appropriate HFC folder
setwd('X:\\Personal Development\\Data Science\\HFC\\norwegian_exchange_rate')
```


```{r}
library(tseries) # Time series analysis
library(timeSeries) # Linear interpolation method for missing data
library(lubridate) # Date and time manipulations
library(forecast) # For fitting an ARIMA model and doing forecasting
```

I will need to specify the end date for the forecast
```{r}
end_date <- ymd('20171031')
```



First off I think I'll import the data... 
```{r}
df <- read.csv('DEXNOUS.csv', stringsAsFactors = FALSE)
```

I will also create a full date range to fill in weekends and holidays where the market would be closed, so that I can develop a fully continuous time series with daily intervals

```{r}
# convert table date to R date object
df$DATE<- mdy(df$DATE)

full_range <- data.frame(DATE = seq(min(df$DATE), max(df$DATE), 'days'))

daily_data <- merge(x = full_range, y = df, by = "DATE", all.x = TRUE)
```

I'll just note that because there are missing observations in the full time series (due to holidays, weekends, or other market closures) I will have to replace them in some fashion. For now, I will use a linear interpolation method until something better occurs to me. This could risk muting some of the true variability of the series, and I could try a few different methods later on and average out the results. 

Another option entirely would be to aggregate to a weekly closing, though this is spoiled a bit by the requirement to forecast a daily closing. Because the 31st is a Tuesday, I could start my time series on a Wednesday so the weekly closing would be the same as the daily closing for my time frame. This could take a good bit more cleaning. Also, I would still be spoiled by weeks where there is a holiday on Tuesday

But ah, I digress. To the time series!! 

```{r}
# Interpolate the missing obs 
daily_data$DEXNOUS <- round(interpNA(as.numeric(daily_data$DEXNOUS), method = "linear"),2)

# Create a time series of object with a frequency of 365 to represent that the data is daily
myts <- ts(daily_data$DEXNOUS, frequency = 365)

plot(myts)
```



## Analysis 1 - Holt-Winters Forecast


Now I will fit an exponential model to the time series using the automatic model specification
```{r}
fit <- HoltWinters(myts)
```

I will use this exponential model to forecast a range of observations 
```{r}
myfcast <- forecast(fit, h = (end_date - max(daily_data$DATE)), level = c(50,75,80,95) )
```

A plot of the forecast 
```{r}
plot(myfcast)
```

Based on the mean and available intervals, we can calculate a standard deviation. 

This can be used to calculate the appropriate mount for the buckets on GJopen 

```{r}
mean <- myfcast$mean[(end_date - max(daily_data$DATE))]

# 1.15 is the z-Score for a 75% confidence interval 
stddev <- as.numeric((myfcast$lower[(end_date - max(daily_data$DATE)),2] - mean)/ (-1.15))
```

Interval 1 
< 7.58
```{r}
z1 <- (7.57 - mean) / stddev
prob1 <- round(pnorm(z1)*100,0)
prob1
```

Interval 2
7.58 <= x <= 7.72
```{r}
z2l <- (7.58 - mean) / stddev
z2u <- (7.72 - mean) / stddev
prob2 <- round(pnorm(z2u)*100,0) - round(pnorm(z2l)*100,0)
prob2
```

Interval 3
7.72 < x < 7.83
```{r}
z3l <- (7.72 - mean) / stddev
z3u <- (7.83 - mean) / stddev
prob3 <- round(pnorm(z3u)*100,0) - round(pnorm(z3l)*100,0)
prob3
```

Interval 4
7.83 <= x <= 7.97
```{r}
z4l <- (7.83 - mean) / stddev
z4u <- (7.97 - mean) / stddev
prob4 <- round(pnorm(z4u)*100,0) - round(pnorm(z4l)*100,0)
prob4
```

Interval 5
x > 7.97
```{r}
z5 <- (7.97 - mean) / stddev
prob5 <- 100 - round(pnorm(z5)*100,0)
prob5
```


Sanity check
```{r}
prob1 + prob2 + prob3 + prob4 + prob5
```


## Analysis 2 - Distribution of Period Movement 

For this analysis I will calculate over-the-period change of a length equivalent to the remaining period in the forecast. I will use the distribution of these changes to determine the likelihood  of each bin.

Calculate the remaining number of days in the period
```{r}
range <- as.numeric(end_date) - as.numeric(today())
```

Assign a monotonic variable for current rates
```{r}
left <- daily_data
left$current_date <- left$DATE
left$DATE <- NULL
left$current_rate <- left$DEXNOUS
left$DEXNOUS <- NULL
left$i <- seq(1:length(left$current_rate))
```

Assign monotonic variable for prior rates
```{r}
right <- daily_data
right$prior_date <- right$DATE
right$DATE <- NULL
right$prior_rate <- right$DEXNOUS
right$DEXNOUS <- NULL
right$i <- seq(1:length(right$prior_rate)) + range - 1
```

Merge the two tables
```{r}
all_df <- merge(x = left, y = right, by = "i")
```

Calculate the OTP Link
```{r}
all_df$otp_link <- all_df$current_rate/all_df$prior_rate
```

Summary of OTP Links
```{r}
mean_link <- mean(all_df$otp_link)
sd_link <- sd(all_df$otp_link)
```




## Simple forecast to today's date

```{r}
paste("Today, the forecast would have predicted a rate of", round(myfcast$mean[(today() - max(daily_data$DATE))],2))
```

